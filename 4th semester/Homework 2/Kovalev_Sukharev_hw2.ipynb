{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Домашнее задание 2. Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работу выполнили Ковалев Евгений и Сухарев Иван, 4 группа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном домашнем задании мы работали с моделью word2vec: обучили ее на отзывах из файла unlabeledTrainData.tsv из соревнования \"Bag of Words Meets Bags of Popcorn\" на Kaggle (https://www.kaggle.com/c/word2vec-nlp-tutorial/data) и опробовали некоторые связанные с ней функции (.most_similar, .doesnt_match) для решения типовых задач (определение близких слов, ассоциаций, лишнего слова)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[1 балл] Обучите модель word2vec. Оцените время обучения модели, используя модуль time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В вышеупомянутом соревновании на Kaggle присутствует Overview (https://www.kaggle.com/c/word2vec-nlp-tutorial), где есть полезные туториалы, которые мы изучили. Для предобработки данных мы использовали описанный в одном из них класс KaggleWord2VecUtility (https://github.com/wendykan/DeepLearningMovies/blob/master/KaggleWord2VecUtility.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KaggleWord2VecUtility(object):\n",
    "    @staticmethod\n",
    "    def review_to_wordlist( review, remove_stopwords=False ):\n",
    "        # Function to convert a document to a sequence of words,\n",
    "        # optionally removing stop words.  Returns a list of words.\n",
    "        review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "        words = review_text.lower().split()\n",
    "        if remove_stopwords:\n",
    "            stops = set(stopwords.words(\"english\"))\n",
    "            words = [w for w in words if not w in stops]\n",
    "        return(words)\n",
    "\n",
    "    # Define a function to split a review into parsed sentences\n",
    "    @staticmethod\n",
    "    def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "        # Function to split a review into parsed sentences. Returns a\n",
    "        # list of sentences, where each sentence is a list of words\n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            if len(raw_sentence) > 0:\n",
    "                sentences.append( KaggleWord2VecUtility.review_to_wordlist( raw_sentence, \\\n",
    "                  remove_stopwords ))\n",
    "        return sentences\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выполним предобработку данных, используя токенизатор punkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/osboxes/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('unlabeledTrainData.tsv', sep='\\t', error_bad_lines=False)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = []\n",
    "for review in data[\"review\"]:\n",
    "    sentences += KaggleWord2VecUtility.review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Time spent: 1.2908729275067647 minutes\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "print(\"Training Word2Vec model...\")\n",
    "start_time = time.time()\n",
    "model = Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=1)\n",
    "print('Time spent: {} minutes'.format((time.time() - start_time) / 60))\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучена!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[2 балла] Приведите 5-10 примеров использования .most_similar для определения близких слов. Корректно ли они найдены? Являются ли синонимами исходного слова?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель справляется с определением близких слов к тем, что указаны ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : woman, lad, lady\n",
      "bird : motorcycle, goat, sheep\n",
      "awesome : amazing, incredible, fantastic\n",
      "cry : laugh, cringe, weep\n",
      "letter : journal, letters, diary\n",
      "leave : stay, reach, accept\n",
      "master : puppet, masters, genius\n",
      "murder : crime, murderer, murders\n",
      "poor : lousy, terrible, weak\n",
      "film : movie, picture, flick\n"
     ]
    }
   ],
   "source": [
    "msWords = 'man, bird, awesome, cry, letter, leave, master, murder, poor, film'.split(', ')\n",
    "for wd in msWords:\n",
    "    closest = model.most_similar(positive = [wd], topn = 3)\n",
    "    print(wd + ' : ' + ', '.join([c[0] for c in closest]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, большинство слов найдено корректно. Однако в некоторых случаях вместо синонимов были найдены антонимы (man-woman, cry-laugh). Забавно, что самое близкое слово к \"bird\" - \"motorcycle\" (мы предположили, что они одинаково надоедливо шумят утром под окном, так что похоже на правду), а к \"master\" - \"puppet\" (возможно, это связано с известной песней группы \"Metallica\" - \"Master of Puppets\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[2 балла] Приведите 5-10 примеров использования .most_similar для определения ассоциаций (А к Б, как В к?). Корректно ли найдены ассоциации?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель справляется с определением ассоциаций, указанным ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day -> light, as night -> dark\n",
      "apple -> fruit, as car -> traffic\n",
      "black -> road, as white -> desert\n",
      "moscow -> russia, as paris -> france\n",
      "see -> eyes, as hear -> ears\n"
     ]
    }
   ],
   "source": [
    "def closeAs(first, tosec, asIt):\n",
    "    cl = model.most_similar([asIt, tosec], [first], topn=3)\n",
    "    return first + ' -> ' + tosec + ', as ' + asIt + ' -> ' + cl[0][0]\n",
    "\n",
    "assoClosest = [['day', 'light', 'night'], ['apple', 'fruit', 'car'], ['black', 'road', 'white'],\n",
    "     ['moscow', 'russia', 'paris'], ['see', 'eyes', 'hear']]\n",
    "\n",
    "for triple in assoClosest:\n",
    "    print(closeAs(triple[0], triple[1], triple[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три (1, 4, 5) очевидные ассоциации найдены, несомненно, корректно, другие две посложнее, но тоже, на наш взгляд, вполне интерпретируемы (яблоко среди фруктов как машина в пробке, черная дорога как белая пустыня, почему бы и нет?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[2 балла] Приведите 5-10 примеров использования .doesnt_match для определения лишнего слова. Корректно ли найдены лишние слова?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как модель справляется с определением лишних слов в наборах, указанных ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potato apple carrot cucumber tomato -> apple\n",
      "earth mars mercury venus sun -> sun\n",
      "moscow paris london warsaw australia berlin -> australia\n",
      "horrible awesome flawless fabulous -> horrible\n",
      "owl sparrow penguin eagle crow -> penguin\n"
     ]
    }
   ],
   "source": [
    "dmatch = 'potato apple carrot cucumber tomato\\n\\\n",
    "earth mars mercury venus sun\\n\\\n",
    "moscow paris london warsaw australia berlin\\n\\\n",
    "horrible awesome flawless fabulous\\n\\\n",
    "owl sparrow penguin eagle crow'.split('\\n')\n",
    "\n",
    "for line in dmatch:\n",
    "    cl = line.split(' ')\n",
    "    ans = model.doesnt_match(cl)\n",
    "    print(line + ' -> ' + ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все корректно. В первом наборе модель нашла фрукт среди овощей, во втором - звезду среди планет, в третьем - страну среди городов, в четвертом - негативную оценку среди позитивных, а в пятом даже определила птицу, которая не умеет летать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[3 балла] Попробуйте найти такие пары и тройки слов, для которых не выполняются/выполняются свойства коммутативности и транзитивности относительно операции определения близких слов.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коммутативные пары слов: guitar-piano, phone-cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('piano', 0.801474928855896),\n",
       " ('orchestra', 0.6634154319763184),\n",
       " ('drums', 0.6366580724716187)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['guitar'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guitar', 0.801474928855896),\n",
       " ('mozart', 0.633202075958252),\n",
       " ('orchestra', 0.6214369535446167)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['piano'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cell', 0.6663293242454529),\n",
       " ('phones', 0.6594644784927368),\n",
       " ('telephone', 0.6335700154304504)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['phone'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('phones', 0.7089791893959045),\n",
       " ('mobile', 0.6965519189834595),\n",
       " ('phone', 0.6663293242454529)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['cell'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоммутативные пары слов: car-train, water-sand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('truck', 0.77348792552948),\n",
       " ('train', 0.7148220539093018),\n",
       " ('bus', 0.6981351375579834)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['car'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bus', 0.7943506836891174),\n",
       " ('helicopter', 0.7562844753265381),\n",
       " ('boat', 0.7497224807739258)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['train'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sand', 0.7181669473648071),\n",
       " ('ocean', 0.7157705426216125),\n",
       " ('fish', 0.7005954384803772)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['water'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mud', 0.7529021501541138),\n",
       " ('trucks', 0.743501603603363),\n",
       " ('ocean', 0.7283374667167664)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['sand'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Транзитивные тройки слов: flick-movie-film, awesome-incredible-amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 0.839878261089325),\n",
       " ('picture', 0.6457217931747437),\n",
       " ('flick', 0.640449583530426)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['film'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.8398780822753906),\n",
       " ('flick', 0.7002795934677124),\n",
       " ('movies', 0.565382182598114)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['movie'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('incredible', 0.7846701145172119),\n",
       " ('awesome', 0.7521750926971436),\n",
       " ('outstanding', 0.6853843331336975)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['amazing'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', 0.7846700549125671),\n",
       " ('exceptional', 0.7585932612419128),\n",
       " ('awesome', 0.7159633636474609)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['incredible'], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нетранзитивные тройки слов: fried-sugar-chocolate, fur-yellow-red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soda', 0.6621962189674377),\n",
       " ('sugar', 0.6265168786048889),\n",
       " ('blanket', 0.6231487393379211)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['chocolate'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coated', 0.6561993956565857),\n",
       " ('chocolate', 0.6265168190002441),\n",
       " ('fried', 0.6079890727996826)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['sugar'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yellow', 0.7565332651138306),\n",
       " ('blue', 0.732541561126709),\n",
       " ('velvet', 0.6416245102882385)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['red'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fur', 0.7782459259033203),\n",
       " ('helmet', 0.7574673891067505),\n",
       " ('red', 0.7565332651138306)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['yellow'], topn = 3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
